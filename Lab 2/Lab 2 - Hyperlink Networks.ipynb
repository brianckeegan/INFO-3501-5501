{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Hyperlink Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Professor [Brian Keegan](https://www.brianckeegan.com)**  \n",
    "**[Department of Information Science, CU Boulder](www.colorado.edu/cmci/academics/information-science)**  \n",
    "This notebook is copyright and made available under the [Apache License v2.0](https://creativecommons.org/licenses/by-sa/4.0/) license.\n",
    "\n",
    "This is the second of five lab notebooks that will explore how to do some introductory data extraction and analysis from Wikipedia data. This lab will extend the methods in the prior lab about analyzing a single article's revision histories and use network science methods to analyze the networks of hyperlinks around a single article. You do not need to be fluent in either to complete the lab, but there are many options for extending the analyses we do here by using more advanced queries and scripting methods.\n",
    "\n",
    "**Acknowledgements**  \n",
    "I'd like to thank the Wikimedia Foundation for the [PAWS system](https://wikitech.wikimedia.org/wiki/PAWS) and [related Wikitech infrastructure](https://wikitech.wikimedia.org/wiki/Main_Page) that this workbook runs within. Yuvi Panda, Aaron Halfaker, Jonathan Morgan, and Dario Taraborelli have all provided crucial support and feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm that basic Python commands work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3\n",
    "b = 4\n",
    "a**b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up all the libraries we'll need to connect to the database, retreive information for analysis, and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Makes the plots appear within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Two fundamental packages for doing data manipulation\n",
    "import numpy as np                   # http://www.numpy.org/\n",
    "import pandas as pd                  # http://pandas.pydata.org/\n",
    "\n",
    "# Two related packages for plotting data\n",
    "import matplotlib.pyplot as plt      # http://matplotlib.org/\n",
    "import seaborn as sb                 # https://stanford.edu/~mwaskom/software/seaborn/\n",
    "\n",
    "# Package for requesting data via the web and parsing resulting JSON\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Two packages for accessing the MySQL server\n",
    "import pymysql                       # http://pymysql.readthedocs.io/en/latest/\n",
    "import os                            # https://docs.python.org/3.4/library/os.html\n",
    "\n",
    "# Packages for analyzing complex networks\n",
    "import networkx as nx                # https://networkx.github.io/\n",
    "import igraph as ig\n",
    "\n",
    "# Setup the code environment to use plots with a white background and DataFrames show more columns and rows\n",
    "sb.set_style('whitegrid')\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name of the article you want to use for the rest of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_title = 'Barack Obama'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network 1 - Hyperlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this lab with examine the hyperlinks among Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the content of the page via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes an article title and returns the list of links in the body of the article. Note that the reason we don't use the \"pagelinks\" table in MySQL or the \"links\" parameter in the API is that this includes links within templates. Articles with templates link to each other forming over-dense clusters in the resulting networks. We only want the links appearing in the body of the text.\n",
    "\n",
    "We pass a request to the API, which returns a JSON-formatted string containing the HTML of the page. We use BeautifulSoup to parse through the HTML tree and extract the non-template links and return them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_page_outlinks(page_title,redirects=1):\n",
    "    # Replace spaces with underscores\n",
    "    #page_title = page_title.replace(' ','_')\n",
    "    \n",
    "    bad_titles = ['Special:','Wikipedia:','Help:','Template:','Category:','International Standard','Portal:','s:','File:']\n",
    "    \n",
    "    # Get the response from the API for a query\n",
    "    # After passing a page title, the API returns the HTML markup of the current article version within a JSON payload\n",
    "    req = requests.get('https://en.wikipedia.org/w/api.php?action=parse&format=json&page={0}&redirects={1}&prop=text&disableeditsection=1&disabletoc=1'.format(page_title,redirects))\n",
    "    \n",
    "    # Read the response into JSON to parse and extract the HTML\n",
    "    json_string = json.loads(req.text)\n",
    "    \n",
    "    # Initialize an empty list to store the links\n",
    "    outlinks_list = [] \n",
    "    \n",
    "    if 'parse' in json_string.keys():\n",
    "        page_html = json_string['parse']['text']['*']\n",
    "\n",
    "        # Parse the HTML into Beautiful Soup\n",
    "        soup = BeautifulSoup(page_html,'lxml')\n",
    "\n",
    "        # Delete tags associated with templates\n",
    "        for tag in soup.find_all('tr'):\n",
    "            tag.replace_with('')\n",
    "\n",
    "        # For each paragraph tag, extract the titles within the links\n",
    "        for para in soup.find_all('p'):\n",
    "            for link in para.find_all('a'):\n",
    "                if link.has_attr('title'):\n",
    "                    title = link['title']\n",
    "                    # Ignore links that aren't interesting\n",
    "                    if all(bad not in title for bad in bad_titles):\n",
    "                        outlinks_list.append(title)\n",
    "\n",
    "        # For each unordered list, extract the titles within the child links\n",
    "        for unordered_list in soup.find_all('ul'):\n",
    "            for item in unordered_list.find_all('li'):\n",
    "                for link in item.find_all('a'):\n",
    "                    if link.has_attr('title'):\n",
    "                        title = link['title']\n",
    "                        # Ignore links that aren't interesting\n",
    "                        if all(bad not in title for bad in bad_titles):\n",
    "                            outlinks_list.append(title)\n",
    "\n",
    "    return outlinks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an example article, showing the first 10 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American English',\n",
       " 'Listen',\n",
       " 'President of the United States',\n",
       " 'African American',\n",
       " 'Contiguous United States',\n",
       " 'Honolulu',\n",
       " 'Columbia University',\n",
       " 'Harvard Law School',\n",
       " 'Harvard Law Review',\n",
       " 'Community organizing']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_outlinks = get_page_outlinks(page_title)\n",
    "page_outlinks[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could write a recursive function like `recursively_get_hyperlink_network` that would crawl the hyperlink network out to an arbitrary distance, but this is becomes exhorbitantly expensive at any depth greater than 1. \n",
    "\n",
    "Here's an example function, but is  not executable to prevent you from harming yourself. :)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def recursively_get_hyperlink_network(seed_page,depth):\n",
    "    neighbors = {}\n",
    "    \n",
    "    if depth < 0:\n",
    "        return neighbors\n",
    "    \n",
    "    neighbors[seed_page] = get_page_outlinks(seed_page)\n",
    "    \n",
    "    for neighbor in neighbors[seed_page]:\n",
    "        neighbors[neighbor] = get_hyperlink_network(neighbor,depth-1)\n",
    "        \n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the 1.5-step ego hyperlink network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, define a simple function to get the 1.5-step ego hyperlink network. The \"ego\" is the seed page you start from, the \"alters\" are the neighbors that the ego links out to. We also get the alters of the alters (2nd order alters), but only include these 2nd order connections if they link to 1st order alters. In other words, the 1.5-step ego hyperlink network are all the pages linked from the seed page and the connections among this set of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hyperlink_alters(seed_page):\n",
    "    # Initialize an empty dictionary to act as an adjacency \"list\"\n",
    "    neighbors = {}\n",
    "    \n",
    "    # Get all the alters for the seed page and store them in the adjacency dictionary\n",
    "    neighbors[seed_page] = get_page_outlinks(seed_page,1)\n",
    "    \n",
    "    # For each of the alters, get their alters and store in the adjacency dictionary\n",
    "    for neighbor in list(set(neighbors[seed_page])): # Don't recrawl duplicates\n",
    "        neighbors[neighbor] = get_page_outlinks(neighbor,0)\n",
    "    \n",
    "    # Initialize an empty graph that we will add nodes and edges into\n",
    "    g = nx.DiGraph()\n",
    "    \n",
    "    # For each entry in the adjacency dictionary, check if the alter's alters are also the seed page's alters\n",
    "    # If they are and the edge is already in the graph, increment the edge weight by one\n",
    "    # If they are but the edge is not already in the graph, add the edge with a weight of one\n",
    "    for article,neighbor_list in neighbors.items():\n",
    "        for neighbor in neighbor_list:\n",
    "            if neighbor in neighbors[seed_page] + [seed_page]:\n",
    "                if g.has_edge(article,neighbor):\n",
    "                    g[article][neighbor]['weight'] += 1\n",
    "                else:\n",
    "                    g.add_edge(article,neighbor,weight=1)\n",
    "    \n",
    "    # Return the weighted graph\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this on an example article and save the resulting graph object to disk. \n",
    "\n",
    "*This step could take more than a minute depending on the number of links and size of the neighboring pages.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the hyperlink network\n",
    "hyperlink_g = get_hyperlink_alters(page_title)\n",
    "\n",
    "# Save the graph to disk to visualize in Gephi\n",
    "nx.write_graphml(hyperlink_g,'hyperlink_{0}.graphml'.format(page_title.replace(' ','_')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute descriptive statistics for the hyperlink network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 494 nodes and 4868 edges in the hyperlink network.\n"
     ]
    }
   ],
   "source": [
    "hg_nodes = hyperlink_g.number_of_nodes()\n",
    "hg_edges = hyperlink_g.number_of_edges()\n",
    "\n",
    "print(\"There are {0} nodes and {1} edges in the hyperlink network.\".format(hg_nodes,hg_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.00% of the possible edges actually exist.\n"
     ]
    }
   ],
   "source": [
    "hg_density = nx.density(hyperlink_g)\n",
    "print('{0:.2%} of the possible edges actually exist.'.format(hg_density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.87% of the edges in the hyperlink network are reciprocated.\n"
     ]
    }
   ],
   "source": [
    "def reciprocity(g):\n",
    "    reciprocated_edges = []\n",
    "    \n",
    "    for (i,j) in g.edges():\n",
    "        if hyperlink_g.has_edge(j,i):\n",
    "            reciprocated_edges.append((i,j))\n",
    "    \n",
    "    return len(reciprocated_edges)/float(g.number_of_edges())\n",
    "\n",
    "hg_reciprocity = reciprocity(hyperlink_g)\n",
    "\n",
    "print('{0:.2%} of the edges in the hyperlink network are reciprocated.'.format(hg_reciprocity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the Wikipedia Game!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the hyperlinks on the article, try to get from the first article to the second article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to navigate from \"Arab League\" to \"Chicago Tribune\" using only hyperlinks.\n",
      "\n",
      "Start at: https://en.wikipedia.org/wiki/Arab_League\n"
     ]
    }
   ],
   "source": [
    "page1,page2 = np.random.choice(list(hyperlink_g.nodes()),2)\n",
    "print(\"Try to navigate from \\\"{0}\\\" to \\\"{1}\\\" using only hyperlinks.\\n\".format(page1,page2))\n",
    "print(\"Start at: https://en.wikipedia.org/wiki/{0}\".format(page1.replace(' ','_')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*No cheating!*\n",
    "\n",
    "After you've played the game a few times, see what an optimal shortest path is. You may get an error indicating there is no shortest path, in which case, try a new pair of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arab League', 'The New York Times', 'Chicago Tribune']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.shortest_path(hyperlink_g,page1,page2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify longest shortest path lengths in the hyperlink network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [shortest path length](https://en.wikipedia.org/wiki/Shortest_path_problem) is the path connecting two nodes in the fewest steps. This is related to the \"small world\" effect where everyone in the world is just a few handshakes from each other. It's rare to find complex networks where the longest shortest path is above 5. Nodes that are this far from each other are likely about very unrelated topics. \n",
    "\n",
    "*If there are no paths greater than 5, lower the `path_length_threshold` from 5 to 4.*\n",
    "\n",
    "The `long_path_lengths` dictionary below is populated by computing all the shortest path lengths between nodes in the network and only keeping those paths that are longer than 5 steps from each other. In a directed graph like our hyperlink network, it's important to follow the direction of the arrows: if page A links to page B but page B doesn't link to page A, then we can't make a shortest path from B to A, we have to find another path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Metropolitan Transportation Authority', 'National Association for Business Economics', 'Indoor tanning'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_length_threshold = 4\n",
    "long_path_lengths = {}\n",
    "\n",
    "for k,d in nx.all_pairs_shortest_path_length(hyperlink_g).items():\n",
    "    long_paths = [v for v,l in d.items() if l > path_length_threshold]\n",
    "    if len(long_paths) > 0:\n",
    "        long_path_lengths[k] = long_paths\n",
    "        \n",
    "long_path_lengths.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest path between the articles can be identified using the `shortest_path` function and supplying the graph and the names of two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two pages randomly selected are: \"Metropolitan Transportation Authority\" and \"Indoor tanning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Metropolitan Transportation Authority',\n",
       " 'New York City Subway',\n",
       " 'World War II',\n",
       " 'NATO',\n",
       " 'Barack Obama',\n",
       " 'Indoor tanning']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly choose two articles in the list of long shortest paths\n",
    "page1,page2 = np.random.choice(list(long_path_lengths.keys()),2)\n",
    "print(\"The two pages randomly selected are: \\\"{0}\\\" and \\\"{1}\\\"\".format(page1,page2))\n",
    "\n",
    "# Display the path between these articles\n",
    "nx.shortest_path(hyperlink_g,page1,page2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out different combinations of articles from the `long_path_lengths` to find the articles that are farthest apart by entering different article names for `page1` and `page2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['National Association for Business Economics',\n",
       " 'Congressional Budget Office',\n",
       " 'United States Congress',\n",
       " 'World War II',\n",
       " 'NATO']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page1 = 'National Association for Business Economics'\n",
    "page2 = 'NATO'\n",
    "nx.shortest_path(hyperlink_g,page1,page2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the most well-connected nodes in hyperlink network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hg_in_degree_d = {node:int(centrality*(len(hyperlink_g) - 1)) for node,centrality in nx.in_degree_centrality(hyperlink_g).items()}\n",
    "hg_out_degree_d = {node:int(centrality*(len(hyperlink_g) - 1)) for node,centrality in nx.out_degree_centrality(hyperlink_g).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the nodes with the highest in-degree: other pages in the network point to this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Barack Obama                              244\n",
       "President of the United States            138\n",
       "The New York Times                        110\n",
       "George W. Bush                            100\n",
       "United States Senate                       98\n",
       "Democratic Party (United States)           94\n",
       "Republican Party (United States)           82\n",
       "Bill Clinton                               78\n",
       "United States Congress                     77\n",
       "United States House of Representatives     74\n",
       "Name: In, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_df = pd.DataFrame({'In':hg_in_degree_d,'Out':hg_out_degree_d})\n",
    "degree_df['In'].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the nodes with the highest-out-degree: these pages point to many other pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Barack Obama                                 493\n",
       "Joe Biden                                     62\n",
       "Hillary Clinton                               57\n",
       "George W. Bush                                52\n",
       "United States presidential election, 2008     44\n",
       "Democratic Party (United States)              44\n",
       "Bill Clinton                                  42\n",
       "Tim Kaine                                     41\n",
       "First inauguration of Barack Obama            41\n",
       "John McCain                                   41\n",
       "Name: Out, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_df['Out'].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the nodes that have no links out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Associate attorney                                   0\n",
       "Calvert School                                       0\n",
       "DMOZ                                                 0\n",
       "Default (finance)                                    0\n",
       "Drilling rig                                         0\n",
       "Foreign student                                      0\n",
       "Glamour (magazine)                                   0\n",
       "Harris Interactive                                   0\n",
       "Hopkins & Sutter                                     0\n",
       "Inaugural address                                    0\n",
       "Joint Political Military Group                       0\n",
       "Listen                                               0\n",
       "Marijuana                                            0\n",
       "Money (magazine)                                     0\n",
       "Nystalus obamai (page does not exist)                0\n",
       "Office of the Vice President of the United States    0\n",
       "Ovarian cancer                                       0\n",
       "President of Cuba                                    0\n",
       "Prisoner exchange                                    0\n",
       "Resurrection of Jesus                                0\n",
       "Tax bracket                                          0\n",
       "Tax incentive                                        0\n",
       "University of Nairobi                                0\n",
       "University-preparatory school                        0\n",
       "Uterine cancer                                       0\n",
       "Wailuku, Hawaii                                      0\n",
       "Name: Out, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_df.query('Out == 0')['Out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at nodes that have a single link in. These are also known as (in-) pendants. If there are none, it should appear as an empty series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137th Street – City College (IRT Broadway – Seventh Avenue Line)    1\n",
       "2005 American League Championship Series                            1\n",
       "2009 Major League Baseball All-Star Game                            1\n",
       "2009 Nobel Peace Prize                                              1\n",
       "Alice Palmer (politician)                                           1\n",
       "Automotive industry crisis of 2008–10                               1\n",
       "Bernie Mac                                                          1\n",
       "Besuki Public School                                                1\n",
       "Business International Corporation                                  1\n",
       "Calvert School                                                      1\n",
       "Car Allowance Rebate System                                         1\n",
       "Chlorine gas                                                        1\n",
       "David D. McKiernan                                                  1\n",
       "Death of Nelson Mandela                                             1\n",
       "Debt ceiling                                                        1\n",
       "Deceptive Practices and Voter Intimidation Prevention Act           1\n",
       "Deepwater drilling                                                  1\n",
       "Democratic Party presidential primaries, 2012                       1\n",
       "Energy policy of the United States                                  1\n",
       "Foreign student                                                     1\n",
       "France 24                                                           1\n",
       "Ghouta chemical attack                                              1\n",
       "Glamour (magazine)                                                  1\n",
       "Hiroshima Peace Memorial Museum                                     1\n",
       "Honest Leadership and Open Government Act                           1\n",
       "Indiana Governor                                                    1\n",
       "Innovation economics                                                1\n",
       "Iraq War De-Escalation Act of 2007                                  1\n",
       "Iraqi insurgency (2011–13)                                          1\n",
       "J-1 visa                                                            1\n",
       "JAMA (journal)                                                      1\n",
       "Joint Political Military Group                                      1\n",
       "Kapiolani Medical Center for Women and Children                     1\n",
       "LGBT American                                                       1\n",
       "Matthew Shepard and James Byrd Jr. Hate Crimes Prevention Act       1\n",
       "National Association of Black Journalists                           1\n",
       "Negotiations leading to the Joint Comprehensive Plan of Action      1\n",
       "Nunn–Lugar Cooperative Threat Reduction                             1\n",
       "Nyang’oma Kogelo                                                    1\n",
       "Nystalus obamai (page does not exist)                               1\n",
       "Ovarian cancer                                                      1\n",
       "Paris Agreement                                                     1\n",
       "Petroleum exploration in the Arctic                                 1\n",
       "Prisoner exchange                                                   1\n",
       "Project Vote                                                        1\n",
       "Public-Private Investment Program for Legacy Assets                 1\n",
       "Reactions to the death of Osama bin Laden                           1\n",
       "Somerville, Massachusetts                                           1\n",
       "Sponsor (legislative)                                               1\n",
       "St. John's Episcopal Church, Lafayette Square                       1\n",
       "Steeler Nation                                                      1\n",
       "United States Senate Committee on Veterans' Affairs                 1\n",
       "United States presidential visits to Sub-Saharan Africa             1\n",
       "University of Nairobi                                               1\n",
       "Virginia Governor                                                   1\n",
       "Voter registration campaign                                         1\n",
       "Westminster Hall                                                    1\n",
       "Name: In, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_df.query('In == 1')['In']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the nodes with a single link out. These are also known as (out-)pendants. If there are none, it should appear as an empty series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anthropology                                                                                       1\n",
       "Audiobook                                                                                          1\n",
       "Cairo University                                                                                   1\n",
       "Christianity Today                                                                                 1\n",
       "Congressional Budget Office                                                                        1\n",
       "Congressional Quarterly                                                                            1\n",
       "Constitutional law                                                                                 1\n",
       "Debt ceiling                                                                                       1\n",
       "Disinvestment from Iran                                                                            1\n",
       "Earmark (politics)                                                                                 1\n",
       "Education in Africa                                                                                1\n",
       "Embryonic stem cell                                                                                1\n",
       "Fiat                                                                                               1\n",
       "Fisher House Foundation                                                                            1\n",
       "Hartford Courant                                                                                   1\n",
       "Hate crime laws in the United States                                                               1\n",
       "Hispanic                                                                                           1\n",
       "Indoor tanning                                                                                     1\n",
       "Innovation economics                                                                               1\n",
       "International Herald Tribune                                                                       1\n",
       "JAMA (journal)                                                                                     1\n",
       "Magna cum laude                                                                                    1\n",
       "Medicare Advantage                                                                                 1\n",
       "Metropolitan Transportation Authority                                                              1\n",
       "National Association for Business Economics                                                        1\n",
       "Norwegian Nobel Committee                                                                          1\n",
       "Of counsel                                                                                         1\n",
       "Paris Agreement                                                                                    1\n",
       "Project Vote                                                                                       1\n",
       "Public-Private Investment Program for Legacy Assets                                                1\n",
       "Resignation from the United States Senate                                                          1\n",
       "Senior Advisor to the President                                                                    1\n",
       "Statute of limitations                                                                             1\n",
       "Steeler Nation                                                                                     1\n",
       "Stimulus (economics)                                                                               1\n",
       "Tax credit                                                                                         1\n",
       "Tebet, South Jakarta                                                                               1\n",
       "Trickle-down economics                                                                             1\n",
       "United States Senate Committee on Environment and Public Works                                     1\n",
       "United States Senate Committee on Veterans' Affairs                                                1\n",
       "United States Senate Foreign Relations Subcommittee on Europe and Regional Security Cooperation    1\n",
       "Voter registration campaign                                                                        1\n",
       "Name: Out, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_df.query('Out == 1')['Out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a page, what are the neighbors that link in to it? Assign a specific article title to the `page1` variable by replacing the `np.random.choice(degree_df.index)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The links into node \"Ben Bernanke\" are:\n",
      "['Barack Obama', 'Income inequality in the United States', 'Evan Bayh', 'Timothy Geithner', 'Dodd–Frank Wall Street Reform and Consumer Protection Act', 'Stimulus (economics)', 'Financial crisis of 2007–08']\n"
     ]
    }
   ],
   "source": [
    "page1 = np.random.choice(degree_df.index)\n",
    "\n",
    "in_connections = hyperlink_g.predecessors(page1)\n",
    "print(\"The links into node \\\"{0}\\\" are:\\n{1}\".format(page1,in_connections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The links out from node \"Ben Bernanke\" are:\n",
      "['Barack Obama', 'Doctor of Philosophy', 'President of the United States', 'Time Person of the Year', 'United States Senate', 'George W. Bush', 'C-SPAN', 'The New York Times', 'The Huffington Post', 'Harvard University', 'Bachelor of Arts', 'United States Congress']\n"
     ]
    }
   ],
   "source": [
    "out_connections = hyperlink_g.successors(page1)\n",
    "print(\"The links out from node \\\"{0}\\\" are:\\n{1}\".format(page1,out_connections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "in_degree_dist_df = degree_df['In'].value_counts().reset_index()\n",
    "out_degree_dist_df = degree_df['Out'].value_counts().reset_index()\n",
    "\n",
    "f,ax = plt.subplots(1,1)\n",
    "in_degree_dist_df.plot.scatter(x='index',y='In',ax=ax,c='blue',label='In')\n",
    "out_degree_dist_df.plot.scatter(x='index',y='Out',ax=ax,c='red',label='Out')\n",
    "ax.set_xscale('symlog')\n",
    "ax.set_yscale('symlog')\n",
    "ax.set_xlim((0,1e3))\n",
    "ax.set_ylim((0,1e3))\n",
    "\n",
    "ax.set_xlabel('Connections')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate communities within the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to compute node community memberships for multiple community detection algorithms within `igraph`. The output is a dictionary of dictionaries where the top-level key is the name of the algorithm and returns a second-level dictionary keyed by the the page name with values being the community membership value. Documentation and details about these algorithms can be found under the `igraph` [graph-class documentation](http://igraph.org/python/doc/igraph.Graph-class.html#community_fastgreedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comparative_community_detector(igraph):\n",
    "    memberships = {}\n",
    "    \n",
    "    # Directed memberships\n",
    "    memberships['betweenness'] = igraph.community_edge_betweenness().as_clustering().membership\n",
    "    memberships['infomap'] = igraph.community_infomap().membership\n",
    "    memberships['spinglass'] = igraph.community_spinglass().membership\n",
    "    memberships['walktrap'] = igraph.community_walktrap().as_clustering().membership\n",
    "    \n",
    "    # Undirected memberships\n",
    "    undirected = igraph.as_undirected()\n",
    "    memberships['fastgreedy'] = undirected.community_fastgreedy().as_clustering().membership\n",
    "    memberships['leading_eigenvector'] = undirected.community_leading_eigenvector().membership\n",
    "    memberships['multilevel'] = undirected.community_multilevel().membership\n",
    "    \n",
    "    labelled_memberships = {}\n",
    "    for label,membership in memberships.items():\n",
    "        labelled_memberships[label] = dict(zip(igraph.vs['id'],membership))\n",
    "        \n",
    "    return labelled_memberships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not included in the `comparative_community_detector` function are two additional community detection algorithms that are too intensive or are not working properly. They're documented below if you ever care to explore in the future."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Uses up a ton of memory and crashes kernel immediately\n",
    "ig_hg_optimal_modularity = hyperlink_g.community_optimal_modularity().membership\n",
    "ig_hg_optimal_modularity_labels = dict(zip(ig_hg.vs['id'],ig_hg_optimal_modularity))\n",
    "pd.Series(ig_hg_optimal_modularity_labels).value_counts().head(10)\n",
    "\n",
    "# Lumps everyone into a single community\n",
    "ig_hg_label_propagation = hyperlink_g.community_label_propagation(initial=range(ig_hg_d.vcount()),fixed=[False]*ig_hg_d.vcount()).membership\n",
    "ig_hg_label_propagation_labels = dict(zip(ig_hg_d.vs['id'],ig_hg_label_propagation))\n",
    "pd.Series(ig_hg_label_propagation_labels).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to shift from using the `networkx` library to using the `igraph` library. The former is built purely in Python which makes it easier-to-use but somewhat slower while the latter is a \"[wrapper](https://en.wikipedia.org/wiki/Wrapper_library)\" that lets us write in Python but does the calculations in much-faster C code behind-the-scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGRAPH D-W- 494 4868 -- \n",
      "+ attr: id (v), weight (e)\n"
     ]
    }
   ],
   "source": [
    "# Load the hyperlink network data from disk into a networkx graph object\n",
    "nx_hg = nx.read_graphml('hyperlink_{0}.graphml'.format(page_title.replace(' ','_')))\n",
    "\n",
    "# Load the hyperlink network data from disk into a igraph graph object\n",
    "ig_hg = ig.read('hyperlink_{0}.graphml'.format(page_title.replace(' ','_')))\n",
    "ig.summary(ig_hg) # Get statistics about the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function on the `igraph` version of the hyperlink network.\n",
    "\n",
    "*This may take a minute or more since these are intensive calculations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in assigning \"Hopkins & Sutter\" to a community.\n"
     ]
    }
   ],
   "source": [
    "# Run the community detection labelling on the igraph graph object\n",
    "comparative_community_labels = comparative_community_detector(ig_hg)\n",
    "\n",
    "# Convert the node labels into a dict-of-dicts keyed by page name and inner-dict containing community labels\n",
    "comparative_community_labels_transposed = pd.DataFrame(comparative_community_labels).to_dict('index')\n",
    "\n",
    "# Update each node in the networkx graph object to reflect the community membership labels\n",
    "for _node in nx_hg.nodes():\n",
    "    try:\n",
    "        nx_hg.node[_node]['label'] = _node\n",
    "        for (label,membership) in comparative_community_labels_transposed[_node].items():\n",
    "            nx_hg.node[_node][label] = int(membership)\n",
    "    except KeyError: # Concerning that some labels aren't present, but skip them for now\n",
    "        print(\"Error in assigning \\\"{0}\\\" to a community.\".format(_node))\n",
    "        pass\n",
    "\n",
    "# Write the labeled graph back to disk to visualize in Gephi\n",
    "nx.write_graphml(nx_hg,'hyperlink_communities_{0}.graphml'.format(page_title.replace(' ','_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
